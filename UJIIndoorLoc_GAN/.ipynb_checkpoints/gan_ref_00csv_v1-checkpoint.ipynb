{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfe20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import clear_session\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74176a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Parameters\n",
    "num_of_classes = 54\n",
    "data_shape = (520,1)\n",
    "\n",
    "#MLP Parameters\n",
    "times_to_run = 30 #Number of times to run MLP model\n",
    "mlp_epochs = 30\n",
    "valid_split = 0.20\n",
    "\n",
    "#GAN Parameters\n",
    "latent_dim = 200\n",
    "gan_epochs = 50\n",
    "\n",
    "#Random Seeds\n",
    "selection_seed = 150\n",
    "seed_multiplier = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f966142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Real Data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12fcb4835d54ac29640621ae9c07ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, max=1.0, min=0.1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints GAN generates:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff5203f15374345b65340bd8805a30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=5, max=40, min=5, step=5),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67e86e19f2849e39934f084a827b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Generate missing data only')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#一个图形化界面\n",
    "cb1 = widgets.Checkbox(description=\"Generate missing data only\")\n",
    "slider1 = widgets.FloatSlider(value=0.2, min=0.1, max=1, step=0.1)\n",
    "slider2 = widgets.IntSlider(value=5, min=5, max=40, step=5)\n",
    "vb = widgets.VBox(children = [slider2])\n",
    "def checkbox(button):\n",
    "    if button['new']:\n",
    "        vb.children = []\n",
    "        slider2.value = 250 - int(slider1.value*250) \n",
    "    else:\n",
    "        vb.children = [slider2]\n",
    "        experiment3 = False\n",
    "cb1.observe(checkbox, names='value')\n",
    "\n",
    "print(\"Percentage of Real Data:\")\n",
    "display(slider1)\n",
    "print(\"Number of datapoints GAN generates:\")\n",
    "display(vb)\n",
    "display(cb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c7e3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_of_data = slider1.value\n",
    "data_to_gen = slider2.value\n",
    "fraction_of_data,data_to_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf75a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0, ..., 53, 53, 53]),\n",
       " (1059, 520),\n",
       " array([[-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        ...,\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110]], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./UJIIndoorLoc/children_13/sorted/00_sorted.csv\")\n",
    "dataset[\"REF\"] = pd.factorize(dataset[\"REF\"])[0].astype(int)#将标签映射到顺序数字上\n",
    "labels = dataset.REF.values\n",
    "features = dataset.drop(columns=['TIMESTAMP','PHONEID','USERID','RELATIVEPOSITION',\n",
    "                                'SPACEID','BUILDINGID','FLOOR','LATITUDE','LONGITUDE',\n",
    "                                'REF','BF']).values\n",
    "labels,features.shape,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ac83da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        ...,\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110],\n",
       "        [-110, -110, -110, ..., -110, -110, -110]], dtype=int64),\n",
       " (847, 520),\n",
       " array([ 3, 13, 45, 14,  6, 22, 15, 10,  8, 29, 48, 26,  9, 36,  0, 19, 13,\n",
       "         5, 20, 44, 18, 44, 34, 17, 19,  4, 11, 45, 13, 51, 26, 22, 13, 53,\n",
       "        27, 32, 44, 25, 50, 46, 28, 19,  5, 32, 45, 49, 16,  8, 24,  9, 46,\n",
       "        39, 51, 13, 15, 42, 14, 34, 17, 12, 40, 42,  7,  4, 15, 40, 24, 37,\n",
       "        28,  6, 21, 37, 35, 16, 27, 27, 31, 50, 37, 52, 24, 41, 21, 42, 38,\n",
       "        35, 53, 31, 46, 29, 29, 47,  5, 26, 18, 12, 35, 22, 17, 20, 20, 23,\n",
       "         1, 41, 33, 42, 17,  5, 50, 25, 25, 13, 21, 21, 24, 17, 27, 12,  3,\n",
       "        18, 12, 24, 14, 38, 21, 45, 25, 25, 13,  8, 25, 25, 27, 23, 39,  7,\n",
       "        14, 50, 33,  4, 19, 33, 28, 46, 47, 26, 17, 22,  5, 51,  7, 52, 39,\n",
       "        24, 52, 34,  2, 21, 50, 28, 53, 45, 46, 10, 52, 17, 46, 27, 38, 51,\n",
       "        46,  6,  6, 26, 36, 32, 52, 41, 18,  6, 40, 16,  9, 42, 13, 31, 47,\n",
       "        32, 15, 40, 20,  8, 40, 51, 33, 20, 27, 13,  6, 15, 22,  1, 34,  4,\n",
       "        51, 26, 14, 25, 51, 14, 37, 16, 26, 46, 30, 33, 14, 49, 20, 34, 48,\n",
       "        22,  6, 19, 13,  6, 28, 33,  2, 41, 39, 21,  5,  4, 30, 13, 25, 16,\n",
       "        31, 13, 16, 39,  9,  7, 26,  1,  0, 24, 28, 46, 46,  4, 24, 45, 37,\n",
       "        16,  2, 47, 31, 34, 41, 25,  9, 21, 20, 34, 32,  4, 32,  3, 43, 49,\n",
       "        33, 44, 50, 24, 46, 48,  6, 51,  6,  9, 49, 45, 12, 20, 27, 20,  0,\n",
       "        10, 27,  3, 24, 37, 39, 30, 38, 29, 35, 13, 17, 47, 11, 53, 49,  2,\n",
       "        13, 23, 31, 50,  0, 16, 48,  9, 13, 36, 40, 10, 28, 49, 28,  7, 30,\n",
       "        17, 24, 30, 40, 32, 39, 37, 35, 30, 26, 25, 22,  4, 20, 50, 27, 15,\n",
       "        35, 13, 11, 15, 41, 14, 26, 32, 52, 48, 51, 45, 49, 51, 52, 21, 19,\n",
       "        53,  5, 46, 15, 53, 14,  7,  0, 37, 25, 45, 12, 32, 21, 35, 18,  4,\n",
       "        22,  4, 40, 45, 36, 47, 39, 11, 14,  0, 53,  4,  5, 25, 25, 35,  6,\n",
       "        36, 12, 37, 31, 46,  1, 28, 52, 35, 33,  0, 10, 20, 26, 26, 13, 20,\n",
       "         9,  4, 19, 32, 38, 14, 13, 28,  7, 40, 43,  5, 25, 30, 12, 51, 34,\n",
       "        40, 43, 31,  0, 29, 21, 15, 32, 48, 14, 30, 51,  0, 12, 45, 20, 26,\n",
       "        52, 40,  0, 20, 25, 18, 25, 17, 38, 42, 35, 30, 26, 33, 28, 27, 28,\n",
       "        32,  1, 36, 33, 25, 12, 47, 38, 39, 16,  8, 30, 38, 13, 21, 34, 44,\n",
       "         7, 34, 48, 11, 36, 52, 32, 24, 12, 33, 15, 29, 18, 13, 33, 46, 47,\n",
       "        31, 25, 33, 24,  9, 49,  4, 30, 11, 49, 24, 18, 43, 15, 13, 19, 47,\n",
       "        26, 41, 25,  6, 29, 10, 24,  9, 20,  7, 34, 44, 42, 29,  8, 11, 19,\n",
       "        38, 29, 16,  2, 47, 53,  7, 50, 45, 18, 21, 41, 29, 42, 53, 50, 49,\n",
       "         3, 27, 38, 51, 39, 30, 17, 47, 15,  6, 23, 14, 34, 22, 44, 40, 32,\n",
       "        52, 37, 27, 34, 13, 40,  5, 49, 19, 19, 50, 20, 39, 43,  1, 36, 53,\n",
       "        35, 16, 37, 31,  5, 49,  7, 23, 26, 37, 44, 41, 50, 22, 48, 31, 13,\n",
       "         5, 44, 44, 30,  2, 27, 46, 36, 41, 21, 18, 22, 49, 53,  0, 52, 44,\n",
       "        13, 11, 11, 42,  3, 15, 29, 12, 12, 53, 37, 41, 51,  0, 40, 42, 13,\n",
       "        34, 13, 38, 51, 26, 53, 40, 45, 47, 36, 29, 44, 44, 44, 48, 26, 25,\n",
       "        17, 20, 32,  8, 12,  4,  0, 31, 15, 23,  2, 36, 37,  0, 35,  0, 16,\n",
       "        38, 48, 26, 15, 26,  0, 22, 36,  5, 32, 22, 11, 33, 48,  6, 10, 43,\n",
       "        18, 34, 52, 42, 38,  7,  5, 47, 48, 51, 22, 16, 33, 19, 10, 41,  2,\n",
       "         8, 53, 16, 41, 52, 13, 15,  7, 39, 13, 34, 22, 53, 24, 19, 39, 16,\n",
       "        11, 20, 30, 20, 14,  9, 15, 37, 35, 40, 45, 11,  4, 49, 25, 31,  8,\n",
       "        14, 48, 12, 39, 23, 36, 14, 45, 52,  8, 48,  1, 42, 41, 36, 33,  1,\n",
       "        31,  8, 26, 13, 47, 27, 29, 12, 31, 12, 50, 27, 26, 29, 21, 42, 44,\n",
       "        18,  3, 26, 49, 36, 13, 49, 20, 50, 53, 12,  5, 13, 17, 41, 21, 11,\n",
       "        47, 12, 51, 28, 25,  6, 45, 31, 13, 19, 38, 17, 13, 50, 42, 28, 18,\n",
       "        29, 25,  8, 18,  7,  8, 43, 38, 12, 51, 13,  7, 19, 20, 46, 16, 41,\n",
       "        52, 47, 35, 44, 36, 26, 30, 50, 22, 43, 28, 29, 28, 18,  4,  6,  5,\n",
       "        30, 20, 42, 48,  3, 35, 39, 37, 38, 35, 17, 20, 42, 23]),\n",
       " (847,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_test, Y_tr, Y_test = train_test_split(features, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state= selection_seed,\n",
    "                                                    #random_state：可以理解为随机数种子，主要是为了复现结果而设置\n",
    "                                                    stratify=labels)#stratify保证测试集中，所有类别的齐全\n",
    "X_tr,X_tr.shape,Y_tr,Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12386b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Z_train = [] #This is the same as X_train, but it's used for training the GAN\n",
    "Y_train = []\n",
    "\n",
    "for idx in range(54):\n",
    "    number_filter = np.where(Y_tr == idx)\n",
    "    X_filtered, Y_filtered = X_tr[number_filter], Y_tr[number_filter]\n",
    "\n",
    "#     num_of_data = (int)(fraction_of_data*X_filtered.shape[0])\n",
    "    num_of_data = 2\n",
    "    RandIndex = np.random.choice(X_filtered.shape[0], \n",
    "                                 num_of_data, \n",
    "                                 replace=False)\n",
    "    Z_train.append(X_filtered[RandIndex])#append添加是将容器看作整体来进行添加，但extend是将容器打碎后添加（加入的只是元素）\n",
    "    X_train.extend(X_filtered[RandIndex])\n",
    "    Y_train.extend(Y_filtered[RandIndex])\n",
    "# X_train,Z_train\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "Y_train = np.asarray(Y_train, dtype=np.float32)\n",
    "# X_train\n",
    "#random.shuffle只能对一维list和两维list进行数据打乱\n",
    "#numpy.random.shuffle可以对列表和数组进行数据打乱\n",
    "#from sklearn.utils import shuffle中的shuffle可以直接打乱\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "#将类别向量映射为二值类别矩阵, 用于应用到以categorical_crossentropy为目标函数的模型中.\n",
    "#to_categorical和pd.get_dummies推荐前者，后者原理更复杂\n",
    "Y_train_encoded = to_categorical(Y_train)\n",
    "Y_test_encoded = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6c576d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 520)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data must be standized using standard scaler before using the MLP.\n",
    "scaler = StandardScaler()  #from sklearn.preprocessing import StandardScaler\n",
    "X_train_transformed = scaler.fit_transform(X_train)\n",
    "X_test_transformed = scaler.fit_transform(X_test)\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f9923",
   "metadata": {},
   "source": [
    "### 只使用fraction_of_data比例的真数据进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00db03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xfp\\AppData\\Local\\Temp\\ipykernel_3412\\2378831021.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(times_to_run)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc0468e60d44a82a0505de1f727aa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_test_loss =[]\n",
    "all_test_acc = []\n",
    "history = []\n",
    "\n",
    "for i in tqdm_notebook(range(times_to_run)):\n",
    "    seed(i*seed_multiplier)#操作级\n",
    "    tf.random.set_seed(i*seed_multiplier)#图级\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(520,), activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(54, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.0002, 0.5),#learning rate, the exponential decay rate for the 1st moment estimates\n",
    "                                              # 学习率      , 一阶矩估计的指数衰减率\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history_temp = model.fit(X_train_transformed,\n",
    "                            Y_train_encoded,\n",
    "                            epochs=mlp_epochs,\n",
    "                            batch_size=32,\n",
    "                            validation_split=valid_split,\n",
    "                            verbose=0)\n",
    "    history.append(history_temp)\n",
    "    test_loss, test_acc = model.evaluate(X_test_transformed, \n",
    "                                         Y_test_encoded, \n",
    "                                         verbose=0)\n",
    "\n",
    "    all_test_acc.append(test_acc)\n",
    "    all_test_loss.append(test_loss)\n",
    "    del(model)\n",
    "    clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9764ed6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1156057757.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [11]\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.plot(epochs, acc, 'bo', label='Training acc')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "trainacc = []\n",
    "trainloss = []\n",
    "valacc = []\n",
    "valloss = []\n",
    "for i in range (len(history)):\n",
    "    trainacc.append(history[i].history['accuracy'])\n",
    "    trainloss.append(history[i].history['loss'])\n",
    "    valacc.append(history[i].history['val_accuracy'])\n",
    "    valloss.append(history[i].history['val_loss'])\n",
    "\n",
    "acc = np.mean(trainacc, axis=0)\n",
    "val_acc = np.mean(valacc, axis=0)\n",
    "loss = np.mean(trainloss, axis=0)\n",
    "val_loss = np.mean(valloss, axis=0)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "os.mkdir('./00csv_v1_result/{}_real{}_gen{}'.format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy for {}%'.format(fraction_of_data*100))\n",
    "plt.legend()#作用是加上图例，很有必要\n",
    "plt.savefig(\"./00csv_v1_result/{}_real{}_gen{}/real_TRandVAL_acc - {}%.png\".format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen,\n",
    "                                                                                    fraction_of_data*100))\n",
    "plt.figure()#创建新图\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss for {}%'.format(fraction_of_data*100))\n",
    "plt.legend()\n",
    "plt.savefig(\"./00csv_v1_result/{}_real{}_gen{}/real_TRandVAL_loss - {}%.png\".format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen,\n",
    "                                                                                    fraction_of_data*100))\n",
    "test_loss = np.mean(all_test_loss, axis=0)\n",
    "test_acc = np.mean(all_test_acc, axis=0)\n",
    "test_loss,test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2bcf4",
   "metadata": {},
   "source": [
    "## 生成合成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))#经典值为0.9\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(data_shape), activation='tanh'))\n",
    "    model.add(Reshape(data_shape))\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    gendata = model(noise)\n",
    "    \n",
    "    return Model(noise, gendata) #keras.models.Model(input,output) 模型起始输入和最终输出 #Sequential()继承于Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=data_shape))#keras.layers.Flatten将数据压成一维的\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))#二分类一般使用sigmoid 和softmax\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    data = Input(shape=data_shape)\n",
    "    validity = model(data)\n",
    "\n",
    "    return Model(data, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, features, batch_size):\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, features.shape[0], batch_size)\n",
    "            data = features[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            gen_data = generator.predict(noise) #generator\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(data, valid)\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_data, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) #损失函数应该还可以改进\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            g_loss = combined.train_on_batch(noise, valid) #combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f189c62",
   "metadata": {},
   "source": [
    "### 训练gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = []\n",
    "\n",
    "for i in tqdm(range(num_of_classes)):\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=Adam(0.0002, 0.5),\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "    generator = build_generator()\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    gendata = generator(noise)\n",
    "    discriminator.trainable = False#禁用了判别器的参数更新\n",
    "    validity = discriminator(gendata)\n",
    "    combined = Model(noise, validity)#连接生成器和判别器\n",
    "    combined.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "#     minimaxscaler = MinMaxScaler((-1,1))#归一化到（-1，1）之间 #from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    Z_train_transformed = scaler.fit_transform(Z_train[i])\n",
    "    Z_train_transformed = np.expand_dims(Z_train_transformed, axis=2)#扩充维度（*，*，1）\n",
    "\n",
    "    train(epochs=gan_epochs,\n",
    "          features=Z_train_transformed,\n",
    "          batch_size=8)\n",
    "        \n",
    "    noise = np.random.normal(0, 1, (data_to_gen, latent_dim))\n",
    "    gen_data_temp = generator.predict(noise)\n",
    "    gen_data_temp = np.asarray(gen_data_temp, dtype=np.float32)\n",
    "    gen_data_temp = np.squeeze(gen_data_temp)#删除维度为1 #（1，2，5）==> （2，5）\n",
    "    gen_data_temp = scaler.inverse_transform(gen_data_temp) #将归一化数据转回来\n",
    "\n",
    "    gen_data.append(gen_data_temp)\n",
    "\n",
    "    clear_session()\n",
    "    #在每折的开头都需要加上clear_session()。否则上一折的训练集成了这一折的验证集，数据泄露。\n",
    "    #同时，不清空的话，那么graph上的node越来越多，内存问题，时间问题都会变得严峻。\n",
    "    #可以有效解决模型的内存占用问题\n",
    "    del(discriminator)\n",
    "    del(generator)\n",
    "    del(combined)\n",
    "\n",
    "gen_data = np.asarray(gen_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_label = []\n",
    "for i in range(54):\n",
    "    gen_label_temp = np.tile(i, data_to_gen)\n",
    "    gen_label.extend(gen_label_temp)\n",
    "\n",
    "gen_label = np.asarray(gen_label, dtype=np.float32)\n",
    "gen_label_encoded = to_categorical(gen_label)\n",
    "gen_data[110-abs(gen_data)<1]=-110 #空值滤波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21735e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_reshaped = gen_data.reshape(num_of_classes*data_to_gen, data_shape[0])\n",
    "X_train_gan, Y_train_gan = shuffle(gen_data_reshaped,\n",
    "                                   gen_label_encoded, \n",
    "                                   random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6aaa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_train = np.concatenate((X_train, X_train_gan), axis=0)\n",
    "new_y_train = np.concatenate((Y_train_encoded, Y_train_gan), axis=0)\n",
    "\n",
    "new_x_train, new_y_train = shuffle(new_x_train, new_y_train, random_state=15)\n",
    "new_x_train_transformed = scaler.fit_transform(new_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f4d8a",
   "metadata": {},
   "source": [
    "## Train a new MLP on Real+Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_loss_gan =[]\n",
    "all_test_acc_gan = []\n",
    "ganhistory = []\n",
    "\n",
    "for i in tqdm_notebook(range(times_to_run)):\n",
    "    seed(i*seed_multiplier)\n",
    "    tf.random.set_seed(i*seed_multiplier)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(520,), activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(54, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.0002, 0.5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    ganhistorytemp = model.fit(new_x_train_transformed,\n",
    "                    new_y_train,\n",
    "                    epochs=mlp_epochs,\n",
    "                    batch_size=32,\n",
    "                    validation_split=valid_split,\n",
    "                    verbose = 0)\n",
    "    ganhistory.append(ganhistorytemp)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test_transformed, \n",
    "                                         Y_test_encoded, \n",
    "                                         verbose=0)\n",
    "    print(\"#{} Test acc:\".format(i), test_acc)\n",
    "\n",
    "    all_test_acc_gan.append(test_acc)\n",
    "    all_test_loss_gan.append(test_loss)\n",
    "    del(model)\n",
    "    clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2224104",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_trainacc = []\n",
    "gan_trainloss = []\n",
    "gan_valacc = []\n",
    "gan_valloss = []\n",
    "for i in range (len(ganhistory)):\n",
    "    gan_trainacc.append(history[i].history['accuracy'])\n",
    "    gan_trainloss.append(history[i].history['loss'])\n",
    "    gan_valacc.append(history[i].history['val_accuracy'])\n",
    "    gan_valloss.append(history[i].history['val_loss'])\n",
    "\n",
    "acc = np.mean(gan_trainacc, axis=0)\n",
    "val_acc = np.mean(gan_valacc, axis=0)\n",
    "loss = np.mean(gan_trainloss, axis=0)\n",
    "val_loss = np.mean(gan_valloss, axis=0)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy for {}%'.format(fraction_of_data*100))\n",
    "plt.legend()#作用是加上图例，很有必要\n",
    "plt.savefig(\"./00csv_v1_result/{}_real{}_gen{}/gen+real_TRandVAL_acc - {}%.png\".format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen,\n",
    "                                                                                    fraction_of_data*100))\n",
    "plt.figure()#创建新图\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss for {}%'.format(fraction_of_data*100))\n",
    "plt.legend()\n",
    "plt.savefig(\"./00csv_v1_result/{}_real{}_gen{}/gen+real_TRandVAL_loss - {}%.png\".format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen,\n",
    "                                                                                    fraction_of_data*100))\n",
    "test_loss = np.mean(all_test_loss, axis=0)\n",
    "test_acc = np.mean(all_test_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "AccMean = np.mean(all_test_acc)\n",
    "LossMean = np.mean(all_test_loss)\n",
    "AccStd = np.std(all_test_acc)\n",
    "LossStd = np.std(all_test_loss)\n",
    "\n",
    "GanAccMean = np.mean(all_test_acc_gan)\n",
    "GanLossMean = np.mean(all_test_loss_gan)\n",
    "GanAccStd = np.std(all_test_acc_gan)\n",
    "GanLossStd = np.std(all_test_loss_gan)\n",
    "\n",
    "lines  = list()\n",
    "lines.append(\"Original Data (Each Class: {} Real):\".format(num_of_data))\n",
    "lines.append(\"Accuracy mean: {}\".format(AccMean))\n",
    "lines.append(\"Loss mean: {}\".format(LossMean))\n",
    "lines.append(\"Accuracy STD: {}\".format(AccStd))\n",
    "lines.append(\"Loss STD: {} \\n\".format(LossStd))\n",
    "lines.append(\"Maximum Accuracy: {}\".format(np.max(all_test_acc)))\n",
    "lines.append(\"Loss of Maximum Accuracy: {}\".format(\n",
    "    all_test_loss[np.argmax(all_test_acc)]))\n",
    "\n",
    "lines.append(\"\\n ================== \\n\")\n",
    "lines.append(\"Original + GAN Data\" +\n",
    "             \"(Each Class: {} Real + {} GAN):\".format(num_of_data, \n",
    "                                                      data_to_gen))\n",
    "lines.append(\"Accuracy mean: {}\".format(GanAccMean))\n",
    "lines.append(\"Loss mean: {}\".format(GanLossMean))\n",
    "lines.append(\"Accuracy STD: {}\".format(GanAccStd))\n",
    "lines.append(\"Loss STD: {} \\n\".format(GanLossStd))\n",
    "lines.append(\"Maximum Accuracy: {}\".format(np.max(all_test_acc_gan)))\n",
    "lines.append(\"Loss of Maximum Accuracy: {}\".format(\n",
    "    all_test_loss_gan[np.argmax(all_test_acc_gan)]))\n",
    "\n",
    "\n",
    "file_dir = \"./00csv_v1_result/{}_real{}_gen{}/test - {}%\".format(now.strftime(\"%Y%m%d-%H%M%S\"),num_of_data,data_to_gen,\n",
    "                                                                                    fraction_of_data*100)\n",
    "with open(file_dir, \"w\") as filehandle:\n",
    "    for items in lines:\n",
    "        filehandle.write('%s\\n' % items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
